{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kxmD9kYf4CPV"
   },
   "source": [
    "# Lab_02 군집화\n",
    "\n",
    "### Context\n",
    "#### Clustering\n",
    "+ k-means Clustering\n",
    "+ Hierarchical Clustering\n",
    "\n",
    "#### Evaluation\n",
    "+ Silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "import copy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "wine = load_wine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번 군집화 실습을 위해 sklearn 내장 데이터인 와인 데이터를 불러오겠습니다.<br>\n",
    "와인 데이터셋은 알콜, 말산, 페놀 등 13개의 변수를 가지고 있으며, 1,2,3 와인 등급을 라벨 데이터로 가지고 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wine.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = wine.data\n",
    "label = wine.target\n",
    "columns = wine.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data, columns = columns)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JIZRKFAJ4UQW"
   },
   "source": [
    "## Clustering\n",
    "클러스터링은 주어진 데이터들의 특성을 고려해 데이터 클러스터를 정의하고, 클러스터를 대표할 수 있는 대표점을 찾는 비지도 학습의 대표적인 알고리즘 입니다.<br>\n",
    "간단히 말해서, 비슷한 특성을 가진 데이터끼리 묶는다고 말할 수 있습니다. \n",
    "\n",
    "### 1. k-Means 클러스터링\n",
    "k-means 클러스터링은 대표적인 클러스터링 알고리즘 중 하나로, 각 클러스터에 할당된 데이터 포인트들의 평균 좌표를 이용해 중심점을 반복적으로 업데이트하며 클러스터를 형성하는 알고리즘 입니다.<br><br>\n",
    "k-means Clustering 알고리즘은 3가지 단계로 이루어집니다.<br>\n",
    "Step 1. 각 데이터 포인트 i 에 대해 가장 가까운\n",
    "    중심점을 찾고, 그 중심점에 해당하는 클러스터를 할당합니다.\n",
    "    <br>가까운 중심점을 찾을 때는, **유클리드 거리**를 사용합니다.<br>\n",
    "Step 2. 할당된 클러스터를 기반으로 새로운 중심점을 계산합니다.\n",
    "    중심점은 클러스터 내부 점들 좌표의 **산술 평균(mean)**\n",
    "    으로 합니다.<br>\n",
    "Step 3. 각 클러스터의 할당이 바뀌지 않을 때까지 반복합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 점과 점사이의 거리를 어떻게 측정할 수 있을까? \n",
    "\n",
    " k-means clustering은 거리 기반 알고리즘이므로 여러가지 방법으로 거리를 측정할 수 있습니다.<br>\n",
    " #### 1. Manhattan Distance - 각 축에 대해 수직으로만 이동하여 계산하는 거리 측정방식\n",
    " $$D(x,y) = {{\\sum_{i=1}^{d}  |x_i - y_i|} } $$\n",
    " ![./Images/Manhattan.png](./img/Manhattan.png)\n",
    " \n",
    " #### 2. Euclidean Distance - 점과 점사이의 가장 짧은 거리를 계산하는 거리 측정방식\n",
    " $$D(x,y) = {\\sqrt{\\sum_{i=1}^{d}  (x_i - y_i)^2} } $$\n",
    " ![./Images/Euclidean.png](./img/Euclidean.png)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 준비\n",
    "wine 데이터는 13개의 컬럼을 가지고 있고, 하나의 데이터(행)는 13개의 차원으로 이루어진 벡터라고 볼 수 있습니다. <br>\n",
    "13차원은 우리 눈으로 볼 수 있도록 표현하기 어려우므로 앞에서 배운 pca를 통해 2차원으로 만들어 시각화할 수 있도록 변환하겠습니다.<br>\n",
    "그 전에 각 변수들의 값의 범위가 서로 다르므로 min-max 정규화를 통해 조정해주겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.\n",
    "# scaler = \n",
    "# data = scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn\n",
    "# pca = \n",
    "# data = pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) 모델 불러오기 및 정의하기\n",
    "클러스터링은 비지도학습이므로 클러스터의 수는 라벨의 수와 관계 없지만, 3개의 군집을 형성하도록 해보겠습니다.<br>\n",
    "k-means 클러스터링은 sklearn의 cluster 패키지에 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.\n",
    "# kmeans = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) 모델 학습하기 (클러스터링을 통한 중심점 찾기)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kmeans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) 클러스터 할당"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) 결과 살펴보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(data[:, 0], data[:, 1], c=cluster, linewidth=1, edgecolor='black')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hierarchical Clustering\n",
    "Hierarchical Clustering은 거리(Distance) 또는 유사도(Similarity)를 기반으로 클러스터를 형성하는 알고리즘 입니다.<br> \n",
    "k-means Clustering과 다르게 클러스터의 수를 설정해 줄 필요가 없으며, 클러스터 형태를 시각적으로 표현해주는 덴드로그램을 통해 적절한 클러스터의 수를 선택할 수 있습니다.<br>\n",
    "Hierachichal Clustering에는 Bottom-Up 방식의 Agglomerative Method와 Top-Down 방식의 Divisive Method로 나뉩니다.<br>\n",
    "이번 단원에서는 Agglomerative Method를 사용해 실습을 진행합니다.\n",
    "<br><br>Agglomerative Method를 사용한 Hierarchical Clustering 알고리즘은 3가지 단계로 이루어집니다.<br>\n",
    "Step 1. 각 데이터 포인트를 클러스터로 할당합니다. (n개의 클러스터)<br>\n",
    "Step 2. 가까운 클러스터끼리 병합합니다.<br>\n",
    "Step 3. 1개의 클러스터가 될 때까지 반복합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 어떻게 가장 가까운 클러스터를 찾을 수 있을까?\n",
    "방금전 거리 측정 방법으로 맨하탄 거리, 유클리디언 거리에 대해 배웠었습니다.<br>\n",
    "k-means에서는 각 클러스터의 중심점 간의 거리로 클러스터간 거리를 계산했었습니다.<br> 이번 수업에서는 새로운 클러스터간 거리를 계산하는 방법에 대해 알아보겠습니다.<br>\n",
    "#### 1. Single Linkage - 두 클러스터 내의 가장 가까운 점 사이의 거리 \n",
    "![Single Linkage](./img/Single.png)\n",
    "#### 2. Complete Linkage - 두 클러스터 내의 가장 먼 점 사이의 거리\n",
    "![Complete Linkage](./img/Complete.png)\n",
    "#### 3. Average Linkage - 두 클러스터 내의 모든 점 사이의 평균 거리\n",
    "![Average Linkage](./img/Average.png)\n",
    "\n",
    "3개 거리 측정 방식의 결과와 차이점을 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Linkage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) 모델 불러오기 및 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.\n",
    "# single_clustering = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) 모델 학습하기 (클러스터링을 통한 중심점 찾기)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single_clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) 클러스터 할당"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single_cluster ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) 결과 살펴보기\n",
    "* 산점도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(data[:,0], data[:,1], c=single_labels)\n",
    "# plt.title('Sklearn Single Linkage Hierarchical Clustering')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 덴드로그램"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "# Hierarchical Clustering의 자식 노드\n",
    "children = single_clustering.children_\n",
    "\n",
    "# 각 자식 노드간의 거리 정보를 가지고 있지 않기 때문에, 균일하게 그리도록 합니다.\n",
    "distance = np.arange(children.shape[0])\n",
    "\n",
    "# 각 클러스터 단계를 포함한 노드의 수 계산\n",
    "no_of_observations = np.arange(2, children.shape[0]+2)\n",
    "\n",
    "# 덴드로그램을 그리기위한 연결 매트릭스를 생성합니다.\n",
    "linkage_matrix = np.column_stack([children, distance, no_of_observations]).astype(float)\n",
    "\n",
    "# 덴드로그램을 그립니다.\n",
    "dendrogram(linkage_matrix, p = len(data), labels = single_cluster, \n",
    "           show_contracted=True, no_labels = True, )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Linkage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) 모델 불러오기 및 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete_clustering = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) 모델 학습하기 (클러스터링을 통한 중심점 찾기)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete_clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) 클러스터 할당"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete_cluster ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) 결과 살펴보기\n",
    "* 산점도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(data[:,0], data[:,1], c=complete_cluster)\n",
    "# plt.title('Sklearn Complete Linkage Hierarchical Clustering')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 덴드로그램"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "# Hierarchical Clustering의 자식 노드\n",
    "children = complete_clustering.children_\n",
    "\n",
    "# 각 자식 노드간의 거리 정보를 가지고 있지 않기 때문에, 균일하게 그리도록 합니다.\n",
    "distance = np.arange(children.shape[0])\n",
    "\n",
    "# 각 클러스터 단계를 포함한 노드의 수 계산\n",
    "no_of_observations = np.arange(2, children.shape[0]+2)\n",
    "\n",
    "# 덴드로그램을 그리기위한 연결 매트릭스를 생성합니다.\n",
    "linkage_matrix = np.column_stack([children, distance, no_of_observations]).astype(float)\n",
    "\n",
    "# 덴드로그램을 그립니다.\n",
    "dendrogram(linkage_matrix, p = len(data), labels = complete_cluster, \n",
    "           show_contracted=True, no_labels = True, )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Linkage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) 모델 불러오기 및 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.\n",
    "# average_clustering = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) 모델 학습하기 (클러스터링을 통한 중심점 찾기)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average_clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) 클러스터 할당"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average_cluster ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) 결과 살펴보기\n",
    "* 산점도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(data[:,0], data[:,1], c=average_cluster)\n",
    "# plt.title('Sklearn Average Linkage Hierarchical Clustering')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 덴드로그램"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "# Hierarchical Clustering의 자식 노드\n",
    "children = average_clustering.children_\n",
    "\n",
    "# 각 자식 노드간의 거리 정보를 가지고 있지 않기 때문에, 균일하게 그리도록 합니다.\n",
    "distance = np.arange(children.shape[0])\n",
    "\n",
    "# 각 클러스터 단계를 포함한 노드의 수 계산\n",
    "no_of_observations = np.arange(2, children.shape[0]+2)\n",
    "\n",
    "# 덴드로그램을 그리기위한 연결 매트릭스를 생성합니다.\n",
    "linkage_matrix = np.column_stack([children, distance, no_of_observations]).astype(float)\n",
    "\n",
    "# 덴드로그램을 그립니다.\n",
    "dendrogram(linkage_matrix, p = len(data), labels = average_cluster, \n",
    "           show_contracted=True, no_labels = True, )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 클러스터링 결과 비교하기\n",
    "1. Single Linkage\n",
    "    + 두 클러스터 내의 가장 가까운 점을 기준으로 클러스터를 합치기 클러스터 사이의 노이즈에 매우 민감한 특성과 구 형태가 아닌 데이터에 대해 클러스터를 잘 형성한다는 특성이 있습니다.\n",
    "    + wine 데이터는 모든 데이터가 연결되어 있는 듯한 분포를 가지고 있기 때문에, 각 클러스터의 경계가 모호한 노이즈가 많은 형태를 띠고 있습니다. <br>Single Linkage가 구 형태가 아닌 데이터에 대해 클러스터를 잘 형성한다는 특성이 있지만, 이러한 데이터의 경우 Single Linkage 방법을 사용하면 좋은 클러스터를 생성하기 어렵습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,4))\n",
    "\n",
    "plt.scatter(data[:,0], data[:,1], c=single_cluster)\n",
    "plt.title('Sklearn Single Linkage Hierarchical Clustering')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Complete Linkage\n",
    "    + 두 클러스터 내에 가장 먼 점을 기준으로 클러스터를 합치기 때문에 클러스터 사이의 노이즈와 이상치에 민감하지 않은 특성이 있습니다.\n",
    "    + 노이즈에 민감하지 않다는 특성을 가진 Complete Linkage가 좋은 성능을 보여주었습니다. \n",
    "3. Average Linkage\n",
    "    + Single Linkage와 Complete Linkage의 중간쯤에 위치한 Average Linkage가 가장 정답에 가까운 클러스터를 형성한 것을 확인할 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(data[:,0], data[:,1], c=complete_cluster)\n",
    "plt.title('Sklearn Complete Linkage Hierarchical Clustering')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(data[:,0], data[:,1], c=average_cluster)\n",
    "plt.title('Sklearn Average Linkage Hierarchical Clustering')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "### Silhouette\n",
    "+ 실루엣 값은 한 클러스터 안의 데이터들이 다른 클러스터와 비교해서 얼마나 비슷한가를 나타냅니다.<br>\n",
    "+ 같은 클러스터 내의 점들간 거리는 가깝고(cohesion) 서로 다른 클러스터 간의 거리는 멀수록(separation) 높은 값을 얻을 수 있습니다.<br>\n",
    "+ 실루엣 값이 1에 근접한다는 것은 같은 클러스터 내의 평균거리가 다른 클러스터와의 평균거리보다 가깝다는 것을 의미합니다.\n",
    "+ 일반적으로 실루엣 값이 0.5보다 크다면 데이터가 잘 클러스터링 되었다는 것을 나타냅니다.\n",
    "\n",
    "실루엣 공식은 다음과 같습니다.\n",
    "$$ S_i = { {(b_i - a_i)} \\over max(a_i, b_i) }$$\n",
    "\n",
    "$$ a_i\\ :\\ 같은\\ 클러스터\\ 내의\\ 모든\\ 점들\\ 간\\ 평균\\ 거리 $$\n",
    "$$ b_i\\ :\\ \\bar d\\ =\\ (i,c)의\\ 최솟값 $$\n",
    "$$ \\bar d\\ =\\ (i,c)\\ :\\ 다른\\ 클러스터\\ c와\\ i번째 데이터 와의\\ 평균\\ 거리$$\n",
    "<br>\n",
    "\n",
    "직관적으로 수식을 이해해보겠습니다. a<sub>i</sub>는 같은 클러스터 내의 데이터 들이 잘 모여있다면 적은 값을 나타내고, b<sub>i</sub>는 각 클러스터들이 멀리 떨어져있다면 큰 값을 나타내게 됩니다.<br>\n",
    "따라서 수식 S<sub>i</sub>에 따르면, 아주 잘 형성된(같은 클러스터는 가깝고 다른 클러스터끼리는 먼) 클러스터 형태일 때 분모는 b<sub>i</sub>이 되고,<br> 분자는 b<sub>i</sub>에서 아주 작은 값인 a<sub>i</sub>가 빠져 1에 가까운 실루엣 값을 얻을 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 가장 좋은 클러스터를 형성하는 클러스터의 수를 찾아보자\n",
    "k-means 클러스터링과 Average Linkage를 사용한 Hierarchical 클러스터링에서 가장 높은 점수의 클러스터 수는 무엇인지 알아보겠습니다.\n",
    "\n",
    "Silhouette 스코어링은 Sklearn의 metrics 패키지에 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import silhouette_score\n",
    "\n",
    "# best_n = 1\n",
    "# best_score = -1\n",
    "\n",
    "# for n_cluster in range(2, 11):\n",
    "#     kmeans = \n",
    "#     kmeans.\n",
    "#     cluster = \n",
    "#     score = \n",
    "    \n",
    "#     print('클러스터의 수 : {}, 실루엣 점수 : {:.2f}'.format(n_cluster, score))\n",
    "#     if score > best_score :\n",
    "#         best_n = n_cluster\n",
    "#         best_score = score\n",
    "        \n",
    "# print('가장 높은 실루엣 점수를 가진 클러스터 수 : {}, 실루엣 점수 : {:.2f}'.format(best_n, best_score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Average Linkage Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import silhouette_score\n",
    "\n",
    "# best_n = 1\n",
    "# best_score = -1\n",
    "\n",
    "# for n_cluster in range(2, 11):\n",
    "#     average_clustering = \n",
    "#     average_clustering.\n",
    "#     cluster = \n",
    "#     score = \n",
    "    \n",
    "#     print('클러스터의 수 : {}, 실루엣 점수 : {:.2f}'.format(n_cluster, score))\n",
    "#     if score > best_score :\n",
    "#         best_n = n_cluster\n",
    "#         best_score = score\n",
    "        \n",
    "# print('가장 높은 실루엣 점수를 가진 클러스터 수 : {}, 실루엣 점수 : {:.2f}'.format(best_n, best_score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "- Wikipedia, Clustering : https://ko.wikipedia.org/wiki/클러스터_분석\n",
    "- Wikipedia, Manhattan distance : https://ko.wikipedia.org/wiki/맨해튼_거리\n",
    "- Wikipedia, Euclidean distance : https://ko.wikipedia.org/wiki/유클리드_거리\n",
    "- Sklearn, Wine dataset : https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html\n",
    "- Sklearn, k-Means Clustering : https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n",
    "- Sklearn, Hierarchical Clustering : https://www.google.com/url?q=http://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html&sa=U&ved=0ahUKEwj_2aiGvt7hAhXLi7wKHei8CNsQFggEMAA&client=internal-uds-cse&cx=016639176250731907682:tjtqbvtvij0&usg=AOvVaw0zVZAVTxgORo-7LbgrNv_o\n",
    "- Sklearn, Silhouette : https://www.google.com/url?q=http://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html&sa=U&ved=0ahUKEwi5lrTZwd7hAhUqCqYKHWCZCTEQFggEMAA&client=internal-uds-cse&cx=016639176250731907682:tjtqbvtvij0&usg=AOvVaw0-ZT8AJZRmR-qNpN-62Ei-\n",
    "- Sklearn, Silhouette Example : https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html#sphx-glr-auto-examples-cluster-plot-kmeans-silhouette-analysis-py\n",
    "- Scipy, Dendrogram : https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.cluster.hierarchy.dendrogram.html"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab_02) Clustering.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "kcs",
   "language": "python",
   "name": "kcs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
